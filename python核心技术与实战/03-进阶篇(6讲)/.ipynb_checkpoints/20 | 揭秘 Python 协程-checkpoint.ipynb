{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 20 | 揭秘 Python 协程\n",
    "date: 2019-12-12 20:50:07\n",
    "tags:\n",
    "- python\n",
    "- 极客时间\n",
    "categories:\n",
    "- python核心技术与实战\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么首先你要明白，什么是协程？\n",
    "\n",
    "协程是实现并发编程的一种方式。一说并发，你肯定想到了多线程 / 多进程模型，没错，多线程 / 多进程，正是解决并发问题的经典模型之一。最初的互联网世界，多线程 / 多进程在服务器并发中，起到举足轻重的作用。\n",
    "\n",
    "随着互联网的快速发展，你逐渐遇到了 C10K 瓶颈，也就是同时连接到服务器的客户达到了一万个。于是很多代码跑崩了，进程上下文切换占用了大量的资源，线程也顶不住如此巨大的压力，这时， NGINX 带着事件循环出来拯救世界了。\n",
    "\n",
    "如果将多进程 / 多线程类比为起源于唐朝的藩镇割据，那么事件循环，就是宋朝加强的中央集权制。事件循环启动一个统一的调度器，让调度器来决定一个时刻去运行哪个任务，于是省却了多线程中启动线程、管理线程、同步锁等各种开销。同一时期的 NGINX，在高并发下能保持低资源低消耗高性能，相比 Apache 也支持更多的并发连接。\n",
    "\n",
    "再到后来，出现了一个很有名的名词，叫做回调地狱（callback hell），手撸过 JavaScript 的朋友肯定知道我在说什么。我们大家惊喜地发现，这种工具完美地继承了事件循环的优越性，同时还能提供 async / await 语法糖，解决了执行性和可读性共存的难题。于是，协程逐渐被更多人发现并看好，也有越来越多的人尝试用 Node.js 做起了后端开发。（讲个笑话，JavaScript 是一门编程语言。）\n",
    "\n",
    "回到我们的 Python。使用生成器，是 Python 2 开头的时代实现协程的老方法了，Python 3.7 提供了新的基于 asyncio 和 async / await 的方法。我们这节课，同样的，跟随时代，抛弃掉不容易理解、也不容易写的旧的基于生成器的方法，直接来讲新方法。\n",
    "\n",
    "我们先从一个爬虫实例出发，用清晰的讲解思路，带你结合实战来搞懂这个不算特别容易理解的概念。之后，我们再由浅入深，直击协程的核心。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从一个爬虫说起\n",
    "爬虫，就是互联网的蜘蛛，在搜索引擎诞生之时，与其一同来到世上。爬虫每秒钟都会爬取大量的网页，提取关键信息后存储在数据库中，以便日后分析。爬虫有非常简单的 Python 十行代码实现，也有 Google 那样的全球分布式爬虫的上百万行代码，分布在内部上万台服务器上，对全世界的信息进行嗅探。\n",
    "\n",
    "话不多说，我们先看一个简单的爬虫例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "OK url_1\n",
      "crawling url_2\n",
      "OK url_2\n",
      "crawling url_3\n",
      "OK url_3\n",
      "crawling url_4\n",
      "OK url_4\n",
      "CPU times: user 9.55 ms, sys: 5.73 ms, total: 15.3 ms\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    " \n",
    "def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    time.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    " \n",
    "def main(urls):\n",
    "    for url in urls:\n",
    "        crawl_page(url)\n",
    " \n",
    "%time main(['url_1', 'url_2', 'url_3', 'url_4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（注意：本节的主要目的是协程的基础概念，因此我们简化爬虫的 scrawl_page 函数为休眠数秒，休眠时间取决于 url 最后的那个数字。）\n",
    "\n",
    "这是一个很简单的爬虫，main() 函数执行时，调取 crawl_page() 函数进行网络通信，经过若干秒等待后收到结果，然后执行下一个。\n",
    "\n",
    "看起来很简单，但你仔细一算，它也占用了不少时间，五个页面分别用了 1 秒到 4 秒的时间，加起来一共用了 10 秒。这显然效率低下，该怎么优化呢？\n",
    "\n",
    "于是，一个很简单的思路出现了——我们这种爬取操作，完全可以并发化。我们就来看看使用协程怎么写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "OK url_1\n",
      "crawling url_2\n",
      "OK url_2\n",
      "crawling url_3\n",
      "OK url_3\n",
      "crawling url_4\n",
      "OK url_4\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    " \n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    " \n",
    "async def main(urls):\n",
    "    for url in urls:\n",
    "        await crawl_page(url)\n",
    "#pycharm这样运行 \n",
    "# asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))\n",
    "#jupyter需要下面这样运行.\n",
    "await main(['url_1', 'url_2', 'url_3', 'url_4'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看到这段代码，你应该发现了，在 Python 3.7 以上版本中，使用协程写异步程序非常简单。\n",
    "\n",
    "首先来看 import asyncio，这个库包含了大部分我们实现协程所需的魔法工具。\n",
    "\n",
    "async 修饰词声明异步函数，于是，这里的 crawl_page 和 main 都变成了异步函数。而调用异步函数，我们便可得到一个协程对象（coroutine object）。\n",
    "\n",
    "举个例子，如果你 print(crawl_page(''))，便会输出<coroutine object crawl_page at 0x000002BEDF141148>，提示你这是一个 Python 的协程对象，而并不会真正执行这个函数。\n",
    "\n",
    "再来说说协程的执行。执行协程有多种方法，这里我介绍一下常用的三种。\n",
    "\n",
    "首先，我们可以通过 await 来调用。await 执行的效果，和 Python 正常执行是一样的，也就是说程序会阻塞在这里，进入被调用的协程函数，执行完毕返回后再继续，而这也是 await 的字面意思。代码中 await asyncio.sleep(sleep_time) 会在这里休息若干秒，await crawl_page(url) 则会执行 crawl_page() 函数。\n",
    "\n",
    "其次，我们可以通过 asyncio.create_task() 来创建任务，这个我们下节课会详细讲一下，你先简单知道即可。\n",
    "\n",
    "最后，我们需要 asyncio.run 来触发运行。asyncio.run 这个函数是 Python 3.7 之后才有的特性，可以让 Python 的协程接口变得非常简单，你不用去理会事件循环怎么定义和怎么使用的问题（我们会在下面讲）。一个非常好的编程规范是，asyncio.run(main()) 作为主程序的入口函数，在程序运行周期内，只调用一次 asyncio.run。\n",
    "\n",
    "这样，你就大概看懂了协程是怎么用的吧。不妨试着跑一下代码，欸，怎么还是 10 秒？\n",
    "\n",
    "10 秒就对了，还记得上面所说的，await 是同步调用，因此， crawl_page(url) 在当前的调用结束之前，是不会触发下一次调用的。于是，这个代码效果就和上面完全一样了，相当于我们用异步接口写了个同步代码。\n",
    "\n",
    "现在又该怎么办呢？\n",
    "\n",
    "其实很简单，也正是我接下来要讲的协程中的一个重要概念，任务（Task）。老规矩，先看代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "crawling url_2\n",
      "crawling url_3\n",
      "crawling url_4\n",
      "OK url_1\n",
      "OK url_2\n",
      "OK url_3\n",
      "OK url_4\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    " \n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    " \n",
    "async def main(urls):\n",
    "    tasks = [asyncio.create_task(crawl_page(url)) for url in urls]\n",
    "    for task in tasks:\n",
    "        await task\n",
    " \n",
    "#pycharm这样运行 \n",
    "# asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))\n",
    "#jupyter需要下面这样运行.\n",
    "await main(['url_1', 'url_2', 'url_3', 'url_4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以看到，我们有了协程对象后，便可以通过 asyncio.create_task 来创建任务。任务创建后很快就会被调度执行，这样，我们的代码也不会阻塞在任务这里。所以，我们要等所有任务都结束才行，用for task in tasks: await task 即可。\n",
    "\n",
    "这次，你就看到效果了吧，结果显示，运行总时长等于运行时间最长的爬虫。\n",
    "\n",
    "当然，你也可以想一想，这里用多线程应该怎么写？而如果需要爬取的页面有上万个又该怎么办呢？再对比下协程的写法，谁更清晰自是一目了然。\n",
    "\n",
    "其实，对于执行 tasks，还有另一种做法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "crawling url_2\n",
      "crawling url_3\n",
      "crawling url_4\n",
      "OK url_1\n",
      "OK url_2\n",
      "OK url_3\n",
      "OK url_4\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    " \n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    " \n",
    "async def main(urls):\n",
    "    tasks = [asyncio.create_task(crawl_page(url)) for url in urls]\n",
    "    await asyncio.gather(*tasks)\n",
    " \n",
    "#pycharm这样运行 \n",
    "# asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))\n",
    "#jupyter需要下面这样运行.\n",
    "await main(['url_1', 'url_2', 'url_3', 'url_4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的代码也很好理解。唯一要注意的是，*tasks 解包列表，将列表变成了函数的参数；与之对应的是， ** dict 将字典变成了函数的参数。\n",
    "\n",
    "另外，asyncio.create_task，asyncio.run 这些函数都是 Python 3.7 以上的版本才提供的，自然，相比于旧接口它们也更容易理解和阅读。\n",
    "\n",
    "### 解密协程运行时\n",
    "说了这么多，现在，我们不妨来深入代码底层看看。有了前面的知识做基础，你应该很容易理解这两段代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before await\n",
      "worker_1 start\n",
      "worker_1 done\n",
      "awaited worker_1\n",
      "worker_2 start\n",
      "worker_2 done\n",
      "awaited worker_2\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    " \n",
    "async def worker_1():\n",
    "    print('worker_1 start')\n",
    "    await asyncio.sleep(1)\n",
    "    print('worker_1 done')\n",
    " \n",
    "async def worker_2():\n",
    "    print('worker_2 start')\n",
    "    await asyncio.sleep(2)\n",
    "    print('worker_2 done')\n",
    " \n",
    "async def main():\n",
    "    print('before await')\n",
    "    await worker_1()\n",
    "    print('awaited worker_1')\n",
    "    await worker_2()\n",
    "    print('awaited worker_2')\n",
    " \n",
    "# pycharm这样运行 \n",
    "# asyncio.run()\n",
    "# jupyter需要下面这样运行.\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before await\n",
      "worker_1 start\n",
      "worker_2 start\n",
      "worker_1 done\n",
      "awaited worker_1\n",
      "worker_2 done\n",
      "awaited worker_2\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    " \n",
    "async def worker_1():\n",
    "    print('worker_1 start')\n",
    "    await asyncio.sleep(1)\n",
    "    print('worker_1 done')\n",
    " \n",
    "async def worker_2():\n",
    "    print('worker_2 start')\n",
    "    await asyncio.sleep(2)\n",
    "    print('worker_2 done')\n",
    " \n",
    "async def main():\n",
    "    task1 = asyncio.create_task(worker_1())\n",
    "    task2 = asyncio.create_task(worker_2())\n",
    "    print('before await')\n",
    "    await task1\n",
    "    print('awaited worker_1')\n",
    "    await task2\n",
    "    print('awaited worker_2')\n",
    " \n",
    "# pycharm这样运行 \n",
    "# asyncio.run()\n",
    "# jupyter需要下面这样运行.\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过，第二个代码，到底发生了什么呢？为了让你更详细了解到协程和线程的具体区别，这里我详细地分析了整个过程。步骤有点多，别着急，我们慢慢来看。\n",
    "\n",
    "1. asyncio.run(main())，程序进入 main() 函数，事件循环开启；\n",
    "2. task1 和 task2 任务被创建，并进入事件循环等待运行；运行到 print，输出 'before await'；\n",
    "3. await task1 执行，用户选择从当前的主任务中切出，事件调度器开始调度 worker_1；\n",
    "4. worker_1 开始运行，运行 print 输出'worker_1 start'，然后运行到 await asyncio.sleep(1)， 从当前任务切出，事件调度器开始调度 worker_2；\n",
    "5. worker_2 开始运行，运行 print 输出 'worker_2 start'，然后运行 await asyncio.sleep(2) 从当前任务切出；\n",
    "6. 以上所有事件的运行时间，都应该在 1ms 到 10ms 之间，甚至可能更短，事件调度器从这个时候开始暂停调度；\n",
    "7. 一秒钟后，worker_1 的 sleep 完成，事件调度器将控制权重新传给 task_1，输出 'worker_1 done'，task_1 完成任务，从事件循环中退出；\n",
    "8. await task1 完成，事件调度器将控制器传给主任务，输出 'awaited worker_1'，·然后在 await task2 处继续等待；\n",
    "9. 两秒钟后，worker_2 的 sleep 完成，事件调度器将控制权重新传给 task_2，输出 'worker_2 done'，task_2 完成任务，从事件循环中退出；\n",
    "10. 主任务输出 'awaited worker_2'，协程全任务结束，事件循环结束。\n",
    "\n",
    "接下来，我们进阶一下。如果我们想给某些协程任务限定运行时间，一旦超时就取消，又该怎么做呢？再进一步，如果某些协程运行时出现错误，又该怎么处理呢？同样的，来看代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, ZeroDivisionError('division by zero'), CancelledError()]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    " \n",
    "async def worker_1():\n",
    "    await asyncio.sleep(1)\n",
    "    return 1\n",
    " \n",
    "async def worker_2():\n",
    "    await asyncio.sleep(2)\n",
    "    return 2 / 0\n",
    " \n",
    "async def worker_3():\n",
    "    await asyncio.sleep(3)\n",
    "    return 3\n",
    " \n",
    "async def main():\n",
    "    task_1 = asyncio.create_task(worker_1())\n",
    "    task_2 = asyncio.create_task(worker_2())\n",
    "    task_3 = asyncio.create_task(worker_3())\n",
    " \n",
    "    await asyncio.sleep(2)\n",
    "    task_3.cancel()\n",
    " \n",
    "    res = await asyncio.gather(task_1, task_2, task_3, return_exceptions=True)\n",
    "    print(res)\n",
    " \n",
    "# pycharm这样运行 \n",
    "# asyncio.run()\n",
    "# jupyter需要下面这样运行.\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以看到，worker_1 正常运行，worker_2 运行中出现错误，worker_3 执行时间过长被我们 cancel 掉了，这些信息会全部体现在最终的返回结果 res 中。\n",
    "\n",
    "不过要注意return_exceptions=True这行代码。如果不设置这个参数，错误就会完整地 throw 到我们这个执行层，从而需要 try except 来捕捉，这也就意味着其他还没被执行的任务会被全部取消掉。为了避免这个局面，我们将 return_exceptions 设置为 True 即可。\n",
    "\n",
    "到这里，发现了没，线程能实现的，协程都能做到。那就让我们温习一下这些知识点，用协程来实现一个经典的生产者消费者模型吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "namedtuple_TokenInfo:1: RuntimeWarning: coroutine 'main' was never awaited\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "producer_1 put a val: 9\n",
      "producer_2 put a val: 5\n",
      "consumer_1 get a val: 9\n",
      "consumer_2 get a val: 5\n",
      "producer_1 put a val: 10\n",
      "producer_2 put a val: 5\n",
      "consumer_1 get a val: 10\n",
      "consumer_2 get a val: 5\n",
      "producer_1 put a val: 4\n",
      "producer_2 put a val: 3\n",
      "consumer_1 get a val: 4\n",
      "consumer_2 get a val: 3\n",
      "producer_1 put a val: 6\n",
      "producer_2 put a val: 3\n",
      "consumer_1 get a val: 6\n",
      "consumer_2 get a val: 3\n",
      "producer_1 put a val: 2\n",
      "producer_2 put a val: 7\n",
      "consumer_1 get a val: 2\n",
      "consumer_2 get a val: 7\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    " \n",
    "async def consumer(queue, id):\n",
    "    while True:\n",
    "        val = await queue.get()\n",
    "        print('{} get a val: {}'.format(id, val))\n",
    "        await asyncio.sleep(1)\n",
    " \n",
    "async def producer(queue, id):\n",
    "    for i in range(5):\n",
    "        val = random.randint(1, 10)\n",
    "        await queue.put(val)\n",
    "        print('{} put a val: {}'.format(id, val))\n",
    "        await asyncio.sleep(1)\n",
    " \n",
    "async def main():\n",
    "    queue = asyncio.Queue()\n",
    " \n",
    "    consumer_1 = asyncio.create_task(consumer(queue, 'consumer_1'))\n",
    "    consumer_2 = asyncio.create_task(consumer(queue, 'consumer_2'))\n",
    " \n",
    "    producer_1 = asyncio.create_task(producer(queue, 'producer_1'))\n",
    "    producer_2 = asyncio.create_task(producer(queue, 'producer_2'))\n",
    " \n",
    "    await asyncio.sleep(10)\n",
    "    consumer_1.cancel()\n",
    "    consumer_2.cancel()\n",
    "    \n",
    "    await asyncio.gather(consumer_1, consumer_2, producer_1, producer_2, return_exceptions=True)\n",
    " \n",
    " \n",
    "# pycharm这样运行 \n",
    "# asyncio.run()\n",
    "# jupyter需要下面这样运行.\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实战：豆瓣近日推荐电影爬虫\n",
    "最后，进入今天的实战环节——实现一个完整的协程爬虫。\n",
    "\n",
    "任务描述：https://movie.douban.com/cinema/later/beijing/ 这个页面描述了北京最近上映的电影，你能否通过 Python 得到这些电影的名称、上映时间和海报呢？这个页面的海报是缩小版的，我希望你能从具体的电影描述页面中抓取到海报。\n",
    "\n",
    "听起来难度不是很大吧？我在下面给出了同步版本的代码和协程版本的代码，通过运行时间和代码写法的对比，希望你能对协程有更深的了解。（注意：为了突出重点、简化代码，这里我省略了异常处理。）\n",
    "\n",
    "不过，在参考我给出的代码之前，你是不是可以自己先动手写一下、跑一下呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6cadc020c5a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} {} {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-6cadc020c5a1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mall_movies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"showing-soon\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0meach_movie\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_movies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"item\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mall_a_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meach_movie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mall_li_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meach_movie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'li'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "def main():\n",
    "    url = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "    init_page = requests.get(url).content\n",
    "    init_soup = BeautifulSoup(init_page, 'lxml')\n",
    " \n",
    "    all_movies = init_soup.find('div', id=\"showing-soon\")\n",
    "    for each_movie in all_movies.find_all('div', class_=\"item\"):\n",
    "        all_a_tag = each_movie.find_all('a')\n",
    "        all_li_tag = each_movie.find_all('li')\n",
    " \n",
    "        movie_name = all_a_tag[1].text\n",
    "        url_to_fetch = all_a_tag[1]['href']\n",
    "        movie_date = all_li_tag[0].text\n",
    " \n",
    "        response_item = requests.get(url_to_fetch).content\n",
    "        soup_item = BeautifulSoup(response_item, 'lxml')\n",
    "        img_tag = soup_item.find('img')\n",
    " \n",
    "        print('{} {} {}'.format(movie_name, movie_date, img_tag['src']))\n",
    " \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aiohttp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32mcell_name\u001b[0m in \u001b[0;36masync-def-wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aiohttp'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    " \n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "async def fetch_content(url):\n",
    "    async with aiohttp.ClientSession(\n",
    "        headers=header, connector=aiohttp.TCPConnector(ssl=False)\n",
    "    ) as session:\n",
    "        async with session.get(url) as response:\n",
    "            return await response.text()\n",
    " \n",
    "async def main():\n",
    "    url = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "    init_page = await fetch_content(url)\n",
    "    init_soup = BeautifulSoup(init_page, 'lxml')\n",
    " \n",
    "    movie_names, urls_to_fetch, movie_dates = [], [], []\n",
    " \n",
    "    all_movies = init_soup.find('div', id=\"showing-soon\")\n",
    "    for each_movie in all_movies.find_all('div', class_=\"item\"):\n",
    "        all_a_tag = each_movie.find_all('a')\n",
    "        all_li_tag = each_movie.find_all('li')\n",
    " \n",
    "        movie_names.append(all_a_tag[1].text)\n",
    "        urls_to_fetch.append(all_a_tag[1]['href'])\n",
    "        movie_dates.append(all_li_tag[0].text)\n",
    " \n",
    "    tasks = [fetch_content(url) for url in urls_to_fetch]\n",
    "    pages = await asyncio.gather(*tasks)\n",
    " \n",
    "    for movie_name, movie_date, page in zip(movie_names, movie_dates, pages):\n",
    "        soup_item = BeautifulSoup(page, 'lxml')\n",
    "        img_tag = soup_item.find('img')\n",
    " \n",
    "        print('{} {} {}'.format(movie_name, movie_date, img_tag['src']))\n",
    " \n",
    "# pycharm这样运行 \n",
    "# asyncio.run()\n",
    "# jupyter需要下面这样运行.\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "到这里，今天的主要内容就讲完了。今天我用了较长的篇幅，从一个简单的爬虫开始，到一个真正的爬虫结束，在中间穿插讲解了 Python 协程最新的基本概念和用法。这里带你简单复习一下。\n",
    "\n",
    "* 协程和多线程的区别，主要在于两点，一是协程为单线程；二是协程由用户决定，在哪些地方交出控制权，切换到下一个任务。\n",
    "* 协程的写法更加简洁清晰，把 async / await 语法和 create_task 结合来用，对于中小级别的并发需求已经毫无压力。\n",
    "* 写协程程序的时候，你的脑海中要有清晰的事件循环概念，知道程序在什么时候需要暂停、等待 I/O，什么时候需要一并执行到底。\n",
    "最后的最后，请一定不要轻易炫技。多线程模型也一定有其优点，一个真正牛逼的程序员，应该懂得，在什么时候用什么模型能达到工程上的最优，而不是自觉某个技术非常牛逼，所有项目创造条件也要上。技术是工程，而工程则是时间、资源、人力等纷繁复杂的事情的折衷。\n",
    "\n",
    "### 思考题\n",
    "最后给你留一个思考题。协程怎么实现回调函数呢？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
